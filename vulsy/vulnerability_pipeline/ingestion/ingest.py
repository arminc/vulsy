"""Ingestion process."""

import logging
from abc import ABC, abstractmethod
from collections.abc import Callable

from opentelemetry import trace

from vulsy import meter, tracer
from vulsy.common import text
from vulsy.common.crypto.sha256 import generate_sha256_hash
from vulsy.common.http.client import get_text
from vulsy.common.simpel import now, now_date, now_epoch_ms
from vulsy.common.types.date import DateString
from vulsy.vulnerability_pipeline.ingestion.models import (
    EndpointRawDataEvent,
    IngestionError,
    Metrics,
    SourceEndpointInformation,
    SourceIngestionMetadata,
    SourceIngestionStatus,
)
from vulsy.vulnerability_pipeline.ingestion.names import SourceName

logger = logging.getLogger(__name__)


class IngestionDataRepository(ABC):
    """Interface for the ingestion data repository."""

    @abstractmethod
    def get_source_metadata(self, source_name: str) -> SourceIngestionMetadata | None:
        """Retrieve metadata for a given source.

        Args:
            source_name: Name of the source to get metadata for.

        Returns:
            SourceIngestMetadata if found, None otherwise.
        """

    @abstractmethod
    def store_source_metadata(self, source_name: str, source_metadata: SourceIngestionMetadata) -> None:
        """Store metadata for a given source.

        Args:
            source_name: Name of the source to store metadata for.
            source_metadata: Metadata to store.
        """

    @abstractmethod
    def find_event(self, source_name: str, hash_value: str) -> EndpointRawDataEvent | None:
        """Find a source endpoint event by its source name and hash.

        Args:
            source_name: Name of the source to search in.
            hash_value: Hash value to search for.

        Returns:
            EndpointRawDataEvent if found, None otherwise.
        """

    @abstractmethod
    def store_event(self, event: EndpointRawDataEvent) -> None:
        """Store a source endpoint event.

        Args:
            event: The event to store.
        """

    @abstractmethod
    def get_metrics(self, source_name: SourceName, date: DateString) -> Metrics | None:
        """Get metrics for the ingestion process."""

    @abstractmethod
    def store_metrics(self, metrics: Metrics) -> None:
        """Store metrics for the ingestion process."""


get_initial_endpoints_changed_list_interface = Callable[[dict], tuple[list[SourceEndpointInformation], dict]]
get_rawdata_interface = Callable[[str], str]
ingest_endpoint_interface = Callable[
    [SourceName, SourceEndpointInformation, IngestionDataRepository, get_rawdata_interface], bool
]


def update_and_store_source_metadata(
    repo: IngestionDataRepository,
    source_name: SourceName,
    source_metadata: SourceIngestionMetadata,
    status: SourceIngestionStatus,
    metadata: dict | None = None,
) -> None:
    """Update source metadata and store it in the repository.

    Args:
        repo: Repository interface for storing ingestion data.
        source_name: Name of the source being processed.
        source_metadata: Current metadata for the source.
        status: Status of the ingestion process.
        metadata: Optional metadata to store if status is successful.
    """
    source_metadata.last_run_end_time = now()
    source_metadata.last_run_status = status
    # Only change metadata if it is successful
    if metadata is not None and status == SourceIngestionStatus.SUCCESS:
        source_metadata.metadata = metadata
    repo.store_source_metadata(source_name, source_metadata)


def ingest_endpoint(
    source_name: SourceName,
    item: SourceEndpointInformation,
    repo: IngestionDataRepository,
    get_text: get_rawdata_interface,
) -> bool:
    """Ingest a single source endpoint and store it in the repository.

    Args:
        source_name: Name of the source being processed.
        item: Source item to process.
        repo: Repository interface for storing ingestion data.
        get_text: Function to retrieve text content from a URL. Has a default implementation.

    Returns:
        True if the item was new, False if it was a duplicate.
    """
    trace.get_current_span().set_attribute("url", item.url)
    last_modified_hash = item.get_hash()
    if last_modified_hash:
        event = repo.find_event(source_name, last_modified_hash)
        if event:
            return False
    data = get_text(item.url)
    if not last_modified_hash:
        last_modified_hash = generate_sha256_hash(text.normalize(data))
        event = repo.find_event(source_name, last_modified_hash)
        if event:
            return False
    event = EndpointRawDataEvent(name=source_name, url=item.url, hash=last_modified_hash, data=data)
    repo.store_event(event)
    return True


def _update_metrics(
    source_name: SourceName, repo: IngestionDataRepository, correct_count: int, new_count: int, error_count: int
) -> None:
    """Update the metrics for the ingestion process.

    Args:
        source_name: Name of the source being processed.
        repo: Repository interface for storing ingestion data.
        correct_count: Number of correct items ingested.
        new_count: Number of new items ingested.
        error_count: Number of errors during ingestion.
    """
    logger.info("Updating metrics for %s", source_name)
    ingestion_counter = meter.create_counter(
        name="ingestion_counter",
        description="Total number of items ingested for one source run",
        unit="1",
    )
    ingestion_new_counter = meter.create_counter(
        name="ingestion_error_counter",
        description="Total number of new items during ingestion",
        unit="1",
    )
    ingestion_error_counter = meter.create_counter(
        name="ingestion_error_counter",
        description="Total number of errors during ingestion",
        unit="1",
    )

    ingestion_counter.add(correct_count, attributes={"source_name": source_name})
    ingestion_new_counter.add(new_count, attributes={"source_name": source_name})
    ingestion_error_counter.add(error_count, attributes={"source_name": source_name})

    date = now_date()
    epoch_ms = now_epoch_ms()
    metrics = repo.get_metrics(source_name, date)
    if metrics is None:
        metrics = Metrics(
            day=date,
            source_name=source_name,
            ts_start=epoch_ms,
            ts_end=epoch_ms,
            ts_data=[[epoch_ms, correct_count, new_count, error_count]],
        )
    else:
        metrics.ts_data.append([epoch_ms, correct_count, new_count, error_count])
    repo.store_metrics(metrics)


@tracer.start_as_current_span("ingest")
def ingest(
    source_name: SourceName,
    repo: IngestionDataRepository,
    get_initial_endpoints_changed_list: get_initial_endpoints_changed_list_interface,
    get_text: get_rawdata_interface = get_text,
    process_endpoint: ingest_endpoint_interface = ingest_endpoint,
) -> None:
    """Ingest data from a source and store it in the repository.

    This function handles the main ingestion workflow:
    1. Retrieves or creates source metadata
    2. Gets list of changed items from source
    3. Processes each changed item
    4. Updates metadata with final status

    Args:
        source_name: Name of the source to ingest from.
        repo: Repository interface for storing ingestion data.
        get_initial_endpoints_changed_list: Function to get list of changed endpoints from source.
        get_text: Function to retrieve text content from a URL. Has a default implementation.
        process_endpoint: Function to process individual source endpoints. Has a default implementation.

    Raises:
        Exception: If any step in the ingestion process fails.
    """
    try:
        total_count = 0
        total_new_count = 0
        total_error_count = 0
        start_time = now()
        source_metadata: SourceIngestionMetadata | None = repo.get_source_metadata(source_name)
        if source_metadata is None:
            source_metadata = SourceIngestionMetadata()
        source_metadata.last_run_start_time = start_time

        endpoints_changed_list, metadata = get_initial_endpoints_changed_list(source_metadata.metadata)

        total_count = len(endpoints_changed_list)
        for endpoint in endpoints_changed_list:
            # As the ingest_endpoint function can be overriden, we start a new span for it here so it is always traced
            with tracer.start_as_current_span("ingest_endpoint"):
                try:
                    logger.info("Processing endpoint %s", endpoint.url)
                    result = process_endpoint(source_name, endpoint, repo, get_text)
                    if result:
                        total_new_count += 1
                except Exception:
                    # When one item fails, we log it and continue with the next item
                    # this way we can ingest as much as possible
                    total_error_count += 1
                    logger.exception("Error processing item %s", endpoint.url)

        _update_metrics(source_name, repo, total_count, total_new_count, total_error_count)
        if total_error_count > 0:
            # Fail the ingestion process
            raise IngestionError("Error ingesting data for %s, there were %s errors", source_name, total_error_count)  # noqa: TRY301

        update_and_store_source_metadata(repo, source_name, source_metadata, SourceIngestionStatus.SUCCESS, metadata)
    except Exception as e:
        logger.exception("Error ingesting data %s", source_name)
        if source_metadata:
            update_and_store_source_metadata(repo, source_name, source_metadata, SourceIngestionStatus.FAILURE)
        raise IngestionError("Error ingesting data for %s", source_name) from e
